{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from graphviz import Digraph\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "logging.basicConfig(filename='./log.txt',level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "\n",
    "    def __init__(self, num_nodes, actn_fxn, next_num_nodes = 1, initial_weight = None, is_last = False, has_bias = False):\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.actn_fxn  = actn_fxn\n",
    "        self.next_num_nodes = next_num_nodes\n",
    "        self.is_last = is_last\n",
    "        self.weight_matrix = None\n",
    "        self.has_bias = has_bias\n",
    "\n",
    "        #cannot have a bias in output layer\n",
    "        if self.has_bias and self.is_last:\n",
    "            logging.critical(\"An output layer cannot have a bias. Cannot Continue\")\n",
    "            exit()\n",
    "        \n",
    "        #cannot have a bias only in a layer (it doesn't make sense)\n",
    "        if self.has_bias and self.num_nodes == 1:\n",
    "            logging.critical(\"A layer cannot have only one node that is a bias\")\n",
    "            exit()\n",
    "        \n",
    "        logging.debug(\"layer object initialized with \\n\\tNumber of nodes: {}\\n\\tActivation Function: {}\\n\\tNext Number of Nodes: {}\\n\\tIs Last: {}\\n\".format(self.num_nodes,self.actn_fxn,self.next_num_nodes,self.is_last))\n",
    "        \n",
    "        if initial_weight is None:\n",
    "            logging.debug('Using default values for the weight matrix')\n",
    "            self.weight_matrix = np.zeros([self.num_nodes, self.next_num_nodes],dtype=float)\n",
    "\n",
    "        elif isinstance(initial_weight, (int,float)):\n",
    "            logging.debug(\"Using the weight value for all weights in the layer\")\n",
    "            self.weight_matrix = (np.zeros(shape = (self.num_nodes, self.next_num_nodes),dtype=float))\n",
    "            self.weight_matrix.fill(float(initial_weight))\n",
    "\n",
    "        elif initial_weight.shape == (self.num_nodes, self.next_num_nodes):\n",
    "            logging.debug(\"Using user provided values\")\n",
    "            self.weight_matrix = np.array(initial_weight, dtype = float)\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            logging.warning('Weight matrix provided is of incompatible size. Using default values')\n",
    "            self.weight_matrix = np.zeros(shape = (self.num_nodes,self.next_num_nodes),dtype = float)\n",
    "\n",
    "        self.input_vector = None\n",
    "        self.output_vector = None\n",
    "\n",
    "    def send(self, input_vector):\n",
    "\n",
    "        #flattened to keep things simple\n",
    "        input_vector = np.array(input_vector).flatten()\n",
    "\n",
    "        suppossed_input_vector_size = self.num_nodes\n",
    "        \n",
    "        if self.has_bias:\n",
    "            suppossed_input_vector_size -= 1\n",
    "            \n",
    "        if input_vector.shape != (suppossed_input_vector_size,):\n",
    "            logging.critical(\"Input vector not of desired size. Cannot continue!\")\n",
    "            return None\n",
    "\n",
    "        temp = []\n",
    "        if self.has_bias:\n",
    "            temp.append(1)\n",
    "        \n",
    "        for i in input_vector:\n",
    "            temp.append(self.actn_fxn(i))\n",
    "        self.input_vector = np.asarray(temp).flatten()\n",
    "\n",
    "        return self.input_vector\n",
    "\n",
    "    def generate(self):\n",
    "\n",
    "        if self.input_vector is None:\n",
    "            logging.critical(\"Input not yet provided. Cannot continue!\")\n",
    "            return None\n",
    "\n",
    "        if self.is_last:\n",
    "            return self.input_vector\n",
    "\n",
    "        \n",
    "        #the input is required to be in a row only\n",
    "        temp_input_matrix = np.reshape([self.actn_fxn(i)  for i in self.input_vector], newshape=(1,self.num_nodes))\n",
    "\n",
    "\n",
    "        output_matrix = np.dot(temp_input_matrix,self.weight_matrix)\n",
    "\n",
    "        self.output_vector = output_matrix.flatten()\n",
    "        \n",
    "        return self.output_vector\n",
    "\n",
    "\n",
    "    def desc(self):\n",
    "        print(\"Number of Nodes: {}\\n Number of Nodes in next layer: {}\\nWeights:\\n {}\\n\"\n",
    "               .format(self.num_nodes, self.next_num_nodes,self.weight_matrix))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class n_ff_network:\n",
    "\n",
    "    def __init__(self, ff_layers, initial_weights = None, has_bias = None, actn_fxn : str = 'relu'):\n",
    "\n",
    "        self.ff_layers = list(np.array(ff_layers).flatten())\n",
    "        self.num_layers = len(self.ff_layers)\n",
    "\n",
    "        if self.num_layers < 2:\n",
    "            logging.critical('Neural Network cannot have less than two layers. Cannot Continue')\n",
    "            exit()\n",
    "        \n",
    "        self.n_layers = []\n",
    "        \n",
    "        self.actn_fxns = {}\n",
    "        \n",
    "        #default activation functions\n",
    "        self.actn_fxns['relu'] = self.relu\n",
    "        self.actn_fxns['identity'] = self.identity\n",
    "        self.actn_fxns['step'] = self.step\n",
    "        self.actn_fxns['sigmoid_bin'] = self.sigmoid_binary\n",
    "        self.actn_fxns['sigmoid_bipo'] = self.sigmoid_bipolar\n",
    "        \n",
    "        #default activation function parameter\n",
    "        self.theta = 0\n",
    "        \n",
    "        try:\n",
    "            #for future purposes\n",
    "            self.actn_fxn = actn_fxn\n",
    "            actn_fxn = self.actn_fxns[actn_fxn]\n",
    "        except KeyError:\n",
    "            logging.warning('{} Activation function not found. Using ReLu instead'.format(actn_fxn))\n",
    "            self.actn_fxn = 'relu'\n",
    "            actn_fxn = self.actn_fxns['relu']\n",
    "            \n",
    "        \n",
    "        \n",
    "        if has_bias is None:\n",
    "            has_bias = [False for i in range(self.num_layers-1)]\n",
    "        elif np.asarray(has_bias).flatten().shape != (self.num_layers-1,):\n",
    "            logging.warning('has_bias is not of compatible size. Using default value')\n",
    "            has_bias = [False for i in range(self.num_layers-1)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if isinstance(initial_weights,(type(None),int,float)):\n",
    "            self.n_layers = [ layer(num_nodes = self.ff_layers[i], next_num_nodes= self.ff_layers[i+1],\n",
    "                                    actn_fxn = actn_fxn, initial_weight= initial_weights, has_bias = has_bias[i])\n",
    "                               for i in range(self.num_layers - 1)]\n",
    "\n",
    "        elif initial_weights.shape[0] == self.num_layers - 1:\n",
    "            self.n_layers = [ layer(num_nodes = self.ff_layers[i], next_num_nodes= self.ff_layers[i+1],\n",
    "                                    actn_fxn = actn_fxn, initial_weight= initial_weights[i], has_bias = has_bias[i])\n",
    "                               for i in range(self.num_layers - 1)]\n",
    "\n",
    "        self.n_layers.append(layer(num_nodes = self.ff_layers[self.num_layers-1],initial_weight=1,\n",
    "                                       next_num_nodes=self.ff_layers[self.num_layers-1], actn_fxn = actn_fxn, is_last = True,\n",
    "                                  has_bias = False))\n",
    "\n",
    "        self.n_layers[0].actn_fxn = self.identity\n",
    "        self.input_vector = None\n",
    "        self.output_vector = None\n",
    "        \n",
    "        #default learning algos\n",
    "        self.learning_algos = {}\n",
    "        self.learning_algos['hebb'] = self.hebb_learn\n",
    "        self.learning_algos['perceptron'] = self.perceptron_learn\n",
    "        self.learning_algos['delta'] = self.delta_learn\n",
    "        self.learning_algos['backprop'] = self.backprop_learn\n",
    "        \n",
    "        #differented functions of activation functions\n",
    "        self.inv_actn_fxns = {}\n",
    "        \n",
    "        self.inv_actn_fxns['identity'] = self.inv_identity\n",
    "        self.inv_actn_fxns['sigmoid_bin'] = self.inv_sigmoid_binary\n",
    "        self.inv_actn_fxns['sigmoid_bipo'] = self.inv_sigmoid_bipolar\n",
    "        \n",
    "\n",
    "    def inv_identity(self,x):\n",
    "            return 1\n",
    "    \n",
    "    def inv_sigmoid_binary(self,x):\n",
    "        return self.sigmoid_binary(x) * (1 - self.sigmoid_binary(x))\n",
    "    \n",
    "    def inv_sigmoid_bipolar(self,x):\n",
    "        return 2 * self.inv_sigmoid_binary(x)\n",
    "    \n",
    "    def identity(self,x):\n",
    "        return x\n",
    "    \n",
    "    def relu(self,x):\n",
    "        return max(0,x)\n",
    "    \n",
    "    def step(self,x):\n",
    "        \n",
    "        if x > self.theta:\n",
    "            return 1\n",
    "        elif x < self.theta:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def sigmoid_bipolar(self,x):\n",
    "        return 2*self.sigmoid_binary(x) - 1\n",
    "    \n",
    "    def sigmoid_binary(self,x):\n",
    "        return  1/(1 + math.e**(-x))\n",
    "    \n",
    "    def send(self, input_vector):\n",
    "        \n",
    "        #checking if the input layer of neural network accepts the input_vector as a valid input vector\n",
    "        return_val = self.n_layers[0].send(input_vector)\n",
    "        \n",
    "        if return_val is None:\n",
    "            logging.critical('Input_vector incompatible. Cannot continue')\n",
    "            return None;\n",
    "        \n",
    "        #flattened to keep things simple\n",
    "        self.input_vector = np.array(input_vector).flatten()\n",
    "        \n",
    "        return self.input_vector\n",
    "    \n",
    "    def generate(self):\n",
    "\n",
    "        curr_input = self.input_vector\n",
    "\n",
    "        \n",
    "        for i in self.n_layers:\n",
    "            i.send(curr_input)\n",
    "            curr_input = i.generate()\n",
    "\n",
    "        self.output_vector = curr_input\n",
    "\n",
    "        return self.output_vector\n",
    "\n",
    "    def desc(self):\n",
    "        for i in range(self.num_layers):\n",
    "            print(\"Layer {}\\n\".format(i+1))\n",
    "            self.n_layers[i].desc()\n",
    "\n",
    "    def show_network(self,comment=\"Neural Network\"):\n",
    "        dot = Digraph(comment)\n",
    "\n",
    "        \n",
    "        #legend\n",
    "        dot.attr(rankdir='LR',ranksep='4')\n",
    "        dot.node(name=\"input_layer\", rank = 'sink',xlabel=\"Input Layer\",label=\"\", fontsize = '12',style=\"filled\",color=\"green\",fixedsize = 'true', shape=\"square\",width=\"0.1\")\n",
    "        dot.node(name=\"output_layer\", xlabel=\"Output Layer\",label=\"\", fontsize = '12',style=\"filled\",color=\"red\",fixedsize = 'true', shape=\"square\",width=\"0.1\")\n",
    "        if self.num_layers > 2:\n",
    "            dot.node(name=\"hidden_layers\", xlabel=\"Hidden Layers\",label=\"\", fontsize = '12',style=\"filled\",color=\"grey\",fixedsize = 'true', shape=\"square\",width=\"0.1\")\n",
    "        \n",
    "        start = 0\n",
    "        for i in range(self.num_layers):\n",
    "            color = \"grey\"\n",
    "            if i == 0:\n",
    "                color = \"green\"\n",
    "            elif i == self.num_layers - 1:\n",
    "                color = \"red\"\n",
    "                \n",
    "            with dot.subgraph(name = 'cluster_'+str(i)) as subdot:\n",
    "                for j in range(self.n_layers[i].num_nodes):\n",
    "                    label_name = 'x' + str(i) + str(j)\n",
    "                    if self.n_layers[i].has_bias and j == 0:\n",
    "                        label_name = '1'\n",
    "                \n",
    "                    subdot.node(name = str(start),label = label_name,color=color, style=\"filled\",rankdir='TB')\n",
    "                    start +=1\n",
    "\n",
    "        start = 0\n",
    "        for i in range(self.num_layers-1):\n",
    "            curr_layer = self.n_layers[i]\n",
    "            next_start = start + curr_layer.num_nodes\n",
    "\n",
    "            tail = start\n",
    "            head = next_start\n",
    "            for j in range(curr_layer.num_nodes):\n",
    "                head = next_start\n",
    "                for k in range(curr_layer.next_num_nodes):\n",
    "                    dot.edge(tail_name = str(tail),head_name = str(head), label = str(curr_layer.weight_matrix[j][k]))\n",
    "                    head += 1\n",
    "                tail += 1\n",
    "\n",
    "            start = next_start\n",
    "\n",
    "        #dot.render('./'+comment+'.png', view = True)\n",
    "        dot.view()\n",
    "        \n",
    "    def backprop_learn(self, arg_dict):\n",
    "        \n",
    "        try:\n",
    "            output_vector = arg_dict['output_vector']\n",
    "        except KeyError:\n",
    "            logging.critical('output_vector argument not provided. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            learning_rate = arg_dict['learning_rate']\n",
    "        except KeyError:\n",
    "            logging.info('learning_rate argument not provided. Using default {} value'.format(1))\n",
    "            learning_rate = 1\n",
    "        \n",
    "        last_hidden_layer = self.n_layers[self.num_layers-2]\n",
    "        output_layer = self.n_layers[self.num_layers-1]\n",
    "        \n",
    "        #calculating delta of the last layer \n",
    "        deltas = []\n",
    "        curr_deltas = []\n",
    "        for j in range(last_hidden_layer.num_nodes):\n",
    "            curr_deltas.append([])\n",
    "            for k in range(output_layer.num_nodes):\n",
    "                \n",
    "                Ok = self.output_vector[k]\n",
    "                tk = output_vector[k]\n",
    "                xk = output_layer.input_vector[k]\n",
    "                #Oj = last_hidden_layer.output_vector[j]\n",
    "                \n",
    "                delta = (Ok - tk)*self.inv_actn_fxns[self.actn_fxn](xk)\n",
    "                curr_deltas[len(curr_deltas)-1].append(delta)\n",
    "        \n",
    "                #last_hidden_layer.weight_matrix[j][k] += learning_rate * delta * Oj\n",
    "            \n",
    "        #calculating deltas of every hidden layer\n",
    "        deltas.append(curr_deltas)\n",
    "        count = 0\n",
    "        for i in reversed(range(self.num_layers-1)):\n",
    "            \n",
    "            curr_layer = self.n_layers[i]\n",
    "            next_layer = self.n_layers[i+1]\n",
    "            \n",
    "            curr_deltas = []\n",
    "            for j in range(curr_layer.num_nodes):\n",
    "                curr_deltas.append([])\n",
    "                for k in range(next_layer.num_nodes):\n",
    "                        \n",
    "                        delta = np.multiply(deltas[count][k],next_layer.weight_matrix[k]).sum()\n",
    "                        delta *= self.inv_actn_fxns[self.actn_fxn](next_layer.input_vector[k])\n",
    "                        \n",
    "                        curr_deltas[len(curr_deltas)-1].append(delta)\n",
    "                        \n",
    "            \n",
    "                        \n",
    "            deltas.append(curr_deltas)            \n",
    "            count += 1\n",
    "            \n",
    "            \n",
    "        #weight updation time\n",
    "            \n",
    "        for i in range(self.num_layers-1):\n",
    "            \n",
    "            curr_layer = self.n_layers[i]\n",
    "            next_layer = self.n_layers[i+1]\n",
    "            \n",
    "            for j in range(curr_layer.num_nodes):\n",
    "                for k in range(next_layer.num_nodes):\n",
    "                    \n",
    "                    Oj = curr_layer.actn_fxn((curr_layer.input_vector[j]))\n",
    "                    curr_layer.weight_matrix[j][k] += learning_rate*deltas[count][j][k]*Oj\n",
    "                    \n",
    "        \n",
    "        \n",
    "    def delta_learn(self, arg_dict):\n",
    "        \n",
    "        try:\n",
    "            output_vector = arg_dict['output_vector']\n",
    "        except KeyError:\n",
    "            logging.critical('output_vector argument not provided. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            learning_rate = arg_dict['learning_rate']\n",
    "        except KeyError:\n",
    "            logging.info('learning_rate argument not provided. Using default {} value'.format(1))\n",
    "            learning_rate = 1\n",
    "            \n",
    "        if self.num_layers > 2:\n",
    "            logging.critical('Delta learning rule currently doesn\\'t support more than two layers. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        logging.debug('delta_learn function: learning_rate: {}'.format(learning_rate))\n",
    "        input_layer = self.n_layers[0]\n",
    "        \n",
    "        print(output_vector, self.output_vector,end=\" \")\n",
    "        for i in range(len(output_vector)):\n",
    "            t = output_vector[i]\n",
    "            for j in range(input_layer.num_nodes):\n",
    "                input_layer.weight_matrix[j][i] -= learning_rate * (t - self.output_vector[i])*self.input_vector[j]\n",
    "                \n",
    "        print(input_layer.weight_matrix[:,0])\n",
    "        \n",
    "        \n",
    "    def perceptron_learn(self, arg_dict):\n",
    "        \n",
    "        try:\n",
    "            output_vector = arg_dict['output_vector']\n",
    "        except KeyError:\n",
    "            logging.critical('output_vector argument not provided. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            learning_rate = arg_dict['learning_rate']\n",
    "        except KeyError:\n",
    "            learning_rate = 1\n",
    "            \n",
    "        try:\n",
    "            self.theta = arg_dict['theta']\n",
    "        except KeyError:\n",
    "            self.theta = 1\n",
    "            \n",
    "        \n",
    "        logging.debug('perceptron_learn function: learning_rate: {} theta: {}'.format(learning_rate,self.theta))\n",
    "        \n",
    "        if self.actn_fxn != self.step:\n",
    "            logging.warning('Perceptron normally uses Step activation function. Currently using {} instead'.format(self.actn_fxn))\n",
    "        \n",
    "        if self.num_layers > 2:\n",
    "            logging.critical('Perceptron rule currently doesn\\'t support more than two layers. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        if np.equal(output_vector, self.output_vector).prod():\n",
    "            logging.debug('Actual and calcualted output equal. Skipping')\n",
    "            return True\n",
    "        \n",
    "        input_layer = self.n_layers[0]\n",
    "        for i in range(len(output_vector)):\n",
    "            output = output_vector[i]\n",
    "            for j in range(input_layer.num_nodes):\n",
    "                input_layer.weight_matrix[j][i] += learning_rate*output*self.input_vector[j]\n",
    "        \n",
    "        return True\n",
    "            \n",
    "    def hebb_learn(self,arg_dict):\n",
    "        \n",
    "        try:\n",
    "            output_vector = arg_dict['output_vector']\n",
    "        \n",
    "        except KeyError:\n",
    "            logging.critical('output_vector argument not provided. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        #hebb only works when there are no hidden layers\n",
    "        if self.num_layers > 2:\n",
    "            logging.critical('Hebb cannot work with any hidden layers. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        input_layer = self.n_layers[0]\n",
    "        for i in range(len(output_vector)):\n",
    "            output = output_vector[i]\n",
    "            for j in range(input_layer.num_nodes):\n",
    "                input_layer.weight_matrix[j][i] += output*self.input_vector[j]\n",
    "        \n",
    "        \n",
    "    def learn(self, learning_algo: str, input_matrix, output_matrix, epochs = 1, **xargs):\n",
    "        \n",
    "        if learning_algo not in self.learning_algos.keys():\n",
    "            logging.critical('{} learning algorithm is not recoginized. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        input_matrix = np.array(input_matrix)\n",
    "        output_matrix = np.array(output_matrix)\n",
    "        \n",
    "        if len(input_matrix.shape) != 2 or len(output_matrix.shape) != 2:\n",
    "            logging.critical('Input and Output matrices should have only two dimensions. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        if input_matrix.shape[0] != output_matrix.shape[0]:\n",
    "            logging.critical('Input and Output matrices have incompatible dimensions. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        input_layer = self.n_layers[0]\n",
    "        output_layer = self.n_layers[self.num_layers-1]\n",
    "        \n",
    "        num_pairs = input_matrix.shape[0]\n",
    "        \n",
    "        if num_pairs < 1:\n",
    "            logging.critical('No input provided. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        if input_layer.send(input_matrix[0]) is None:\n",
    "            logging.critical('Input Matrix dimensions` incompatible. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        if output_layer.send(output_matrix[0]) is None:\n",
    "            logging.critical('Output Matrix dimensions` incompatible. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        logging.debug('learn function: epochs: {}'.format(epochs))\n",
    "            \n",
    "        \n",
    "        for i in range(epochs):\n",
    "            for j in range(num_pairs):\n",
    "                self.send(input_matrix[j].flatten())\n",
    "                self.generate()\n",
    "                xargs['output_vector'] = output_matrix[j].flatten()\n",
    "                self.learning_algos[learning_algo](xargs)\n",
    "                #print(self.output_vector,output_matrix[j])\n",
    "            #print(\"---------------------------------\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00519303]\n",
      "[[-1], [1], [1], [-1]]\n",
      "[[1, 1], [1, -1], [-1, 1], [-1, -1]]\n"
     ]
    }
   ],
   "source": [
    "network = n_ff_network([2,2,1],actn_fxn='sigmoid_bipo', initial_weights=0.1)\n",
    "def transform_bipolar(val : list) -> list:                                      \n",
    "    ans = []                                                                    \n",
    "    for i in val:                                                               \n",
    "        if i:                                                                   \n",
    "            ans.append(1)                                                       \n",
    "        else:                                                                   \n",
    "            ans.append(-1)                                                      \n",
    "    return ans                                                                  \n",
    "                                                                                \n",
    "                                                                                \n",
    "                                                                                \n",
    "def generate(func):                                                             \n",
    "                                                                                \n",
    "    input_v = []                                                                \n",
    "    output_v = []                                                               \n",
    "                                                                                \n",
    "    x = True                                                                    \n",
    "    y = True                                                                    \n",
    "                                                                                \n",
    "    func = func.lower()                                                         \n",
    "                                                                                \n",
    "    for i in range(4):                                                          \n",
    "                                                                                \n",
    "        if func == 'and':                                                       \n",
    "            t = x and y                                                         \n",
    "        elif func == 'nand':                                                    \n",
    "            t = not (x and y)                                                   \n",
    "        elif func == 'or':                                                      \n",
    "            t = x or y                                                          \n",
    "        elif func == 'nor':                                                     \n",
    "            t = not ( x or y )                                                  \n",
    "        \n",
    "        elif func == 'andnot':\n",
    "            t = x and (not y)\n",
    "        elif func == 'xor':\n",
    "            t = (x != y)\n",
    "        else:                                                                   \n",
    "            raise Exception('not defined')                                      \n",
    "                                                                                \n",
    "        input_v.append(transform_bipolar([x,y]))                           \n",
    "        output_v.append(transform_bipolar([t]))                                 \n",
    "                                                                                \n",
    "        if not y:                                                               \n",
    "            x = not x                                                           \n",
    "        y = not y                                                               \n",
    "                                                                                \n",
    "                                                                                \n",
    "    return input_v, output_v                                                    \n",
    "\n",
    "input_matrix, output_matrix = generate('xor')\n",
    "network.learn('backprop',input_matrix, output_matrix, learning_rate = 0.2, epochs = 100)\n",
    "network.show_network()\n",
    "print(network.output_vector)\n",
    "print(output_matrix)\n",
    "print(input_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.049\n"
     ]
    }
   ],
   "source": [
    "print(\"1.04949494949\"[:-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [1 2]\n"
     ]
    }
   ],
   "source": [
    "print(type(a),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [1, 2]\n"
     ]
    }
   ],
   "source": [
    "b = [1,2]\n",
    "print(type(b),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if print(np.equal(a,b).prod()):\n",
    "    print(\"No so much\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304af3208>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304af3208>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304d34748>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304d34748>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304d34470>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304d34470>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304d34438>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304d34438>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304af3a90>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.identity of <__main__.n_ff_network object at 0x7f3304af3a90>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "DEBUG:root:Actual and calcualted output equal. Skipping\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using identity instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304af3f28>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304af3f28>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:Actual and calcualted output equal. Skipping\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304af3390>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304af3390>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:Actual and calcualted output equal. Skipping\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304af3fd0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304af3fd0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304af31d0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304af31d0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a0f978>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a0f978>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a119e8>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a119e8>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f33446c20f0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f33446c20f0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a343c8>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a343c8>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a34208>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a34208>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a2f5c0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a2f5c0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304d47ac8>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304d47ac8>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a2f940>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using default values for the weight matrix\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.step of <__main__.n_ff_network object at 0x7f3304a2f940>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "INFO:root:learning_rate argument not provided. Using default 1 value\n",
      "\n",
      "INFO:root:theta argument not provided. Using default 0 value\n",
      "\n",
      "WARNING:root:Perceptron normally uses Step activation function. Currently using step instead\n",
      "\n",
      "DEBUG:root:Actual and calcualted output equal. Skipping\n",
      "\n",
      "WARNING:root:sigmoid Activation function not found. Using ReLu instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.relu of <__main__.n_ff_network object at 0x7f3304a2f048>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.relu of <__main__.n_ff_network object at 0x7f3304a2f048>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "WARNING:root:sigmoid Activation function not found. Using ReLu instead\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 3\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.relu of <__main__.n_ff_network object at 0x7f3304a0f4e0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: False\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:layer object initialized with \n",
      "\n",
      "\tNumber of nodes: 1\n",
      "\n",
      "\tActivation Function: <bound method n_ff_network.relu of <__main__.n_ff_network object at 0x7f3304a0f4e0>>\n",
      "\n",
      "\tNext Number of Nodes: 1\n",
      "\n",
      "\tIs Last: True\n",
      "\n",
      "\n",
      "\n",
      "DEBUG:root:Using the weight value for all weights in the layer\n",
      "\n",
      "DEBUG:root:learn function: epochs: 6\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n",
      "DEBUG:root:delta_learn function: learning_rate: 0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open('log.txt')\n",
    "for i in file:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in reversed(range(3-1)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2] [1 2]\n"
     ]
    }
   ],
   "source": [
    "print(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100\n",
      "-99\n",
      "-98\n",
      "-97\n",
      "-96\n",
      "-95\n",
      "-94\n",
      "-93\n",
      "-92\n",
      "-91\n",
      "-90\n",
      "-89\n",
      "-88\n",
      "-87\n",
      "-86\n",
      "-85\n",
      "-84\n",
      "-83\n",
      "-82\n",
      "-81\n",
      "-80\n",
      "-79\n",
      "-78\n",
      "-77\n",
      "-76\n",
      "-75\n",
      "-74\n",
      "-73\n",
      "-72\n",
      "-71\n",
      "-70\n",
      "-69\n",
      "-68\n",
      "-67\n",
      "-66\n",
      "-65\n",
      "-64\n",
      "-63\n",
      "-62\n",
      "-61\n",
      "-60\n",
      "-59\n",
      "-58\n",
      "-57\n",
      "-56\n",
      "-55\n",
      "-54\n",
      "-53\n",
      "-52\n",
      "-51\n",
      "-50\n",
      "-49\n",
      "-48\n",
      "-47\n",
      "-46\n",
      "-45\n",
      "-44\n",
      "-43\n",
      "-42\n",
      "-41\n",
      "-40\n",
      "-39\n",
      "-38\n",
      "-37\n",
      "-36\n",
      "-35\n",
      "-34\n",
      "-33\n",
      "-32\n",
      "-31\n",
      "-30\n",
      "-29\n",
      "-28\n",
      "-27\n",
      "-26\n",
      "-25\n",
      "-24\n",
      "-23\n",
      "-22\n",
      "-21\n",
      "-20\n",
      "-19\n",
      "-18\n",
      "-17\n",
      "-16\n",
      "-15\n",
      "-14\n",
      "-13\n",
      "-12\n",
      "-11\n",
      "-10\n",
      "-9\n",
      "-8\n",
      "-7\n",
      "-6\n",
      "-5\n",
      "-4\n",
      "-3\n",
      "-2\n",
      "-1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "for i in range(-100,100,1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10,10,0.1)\n",
    "plt.plot(x,[network.sigmoid_binary(i) for i in x])\n",
    "plt.plot(x,[network.inv_sigmoid_binary(i) for i in x])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from graphviz import Digraph\n",
    "import math\n",
    "\n",
    "logging.basicConfig(filename='./log.txt',level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_bipolar(x):\n",
    "    return sigmoid_binary(x) - 0.5\n",
    "def sigmoid_binary(x):\n",
    "    return  1/(1 + math.e**(-x))\n",
    "def relu(x):\n",
    "    logging.debug(\"Relu({}) Called: returning {}\".format(x,max(x,0)))\n",
    "    return max(0,x)\n",
    "\n",
    "def identity(x):\n",
    "    logging.debug(\"Identity({}) Called: returning {}\".format(x,x))\n",
    "    return x\n",
    "\n",
    "def step(x,theta):\n",
    "    ret = 0\n",
    "    if x > theta:\n",
    "        ret = 1\n",
    "    elif x < theta:\n",
    "        ret = -1\n",
    "    else:\n",
    "        ret = 0\n",
    "    logging.debug(\"Step({},{}) Called: returning {}\".format(x,theta,ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "\n",
    "    def __init__(self, num_nodes, next_num_nodes = 1, initial_weight = None, is_last = False, has_bias = False, actn_fxn = relu):\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.actn_fxn  = actn_fxn\n",
    "        self.next_num_nodes = next_num_nodes\n",
    "        self.is_last = is_last\n",
    "        self.weight_matrix = None\n",
    "        self.has_bias = has_bias\n",
    "\n",
    "        #cannot have a bias in output layer\n",
    "        if self.has_bias and self.is_last:\n",
    "            logging.critical(\"An output layer cannot have a bias. Cannot Continue\")\n",
    "            exit()\n",
    "        \n",
    "        #cannot have a bias only in a layer (it doesn't make sense)\n",
    "        if self.has_bias and self.num_nodes == 1:\n",
    "            logging.critical(\"A layer cannot have only one node that is a bias\")\n",
    "            exit()\n",
    "        \n",
    "        logging.debug(\"layer object initialized with \\n\\tNumber of nodes: {}\\n\\tActivation Function: {}\\n\\tNext Number of Nodes: {}\\n\\tIs Last: {}\\n\".format(self.num_nodes,self.actn_fxn,self.next_num_nodes,self.is_last))\n",
    "        \n",
    "        if initial_weight is None:\n",
    "            logging.debug('Using default values for the weight matrix')\n",
    "            self.weight_matrix = np.zeros([self.num_nodes, self.next_num_nodes],dtype=float)\n",
    "\n",
    "        elif isinstance(initial_weight, (int,float)):\n",
    "            logging.debug(\"Using the weight value for all weights in the layer\")\n",
    "            self.weight_matrix = (np.zeros(shape = (self.num_nodes, self.next_num_nodes),dtype=float))\n",
    "            self.weight_matrix.fill(float(initial_weight))\n",
    "\n",
    "        elif initial_weight.shape == (self.num_nodes, self.next_num_nodes):\n",
    "            logging.debug(\"Using user provided values\")\n",
    "            self.weight_matrix = np.array(initial_weight, dtype = float)\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            logging.warning('Weight matrix provided is of incompatible size. Using default values')\n",
    "            self.weight_matrix = np.zeros(shape = (self.num_nodes,self.next_num_nodes),dtype = float)\n",
    "\n",
    "        self.input_vector = None\n",
    "\n",
    "    def send(self, input_vector):\n",
    "\n",
    "        #flattened to keep things simple\n",
    "        input_vector = np.array(input_vector).flatten()\n",
    "\n",
    "        suppossed_input_vector_size = self.num_nodes\n",
    "        \n",
    "        if self.has_bias:\n",
    "            suppossed_input_vector_size -= 1\n",
    "            \n",
    "        if input_vector.shape != (suppossed_input_vector_size,):\n",
    "            logging.critical(\"Input vector not of desired size. Cannot continue!\")\n",
    "            return None\n",
    "\n",
    "        temp = []\n",
    "        if self.has_bias:\n",
    "            temp.append(1)\n",
    "        \n",
    "        for i in input_vector:\n",
    "            temp.append(self.actn_fxn(i))\n",
    "        self.input_vector = np.asarray([ self.actn_fxn(i) for i in input_vector ])\n",
    "\n",
    "        return self.input_vector\n",
    "\n",
    "    def generate(self):\n",
    "\n",
    "        if self.input_vector is None:\n",
    "            logging.critical(\"Input not yet provided. Cannot continue!\")\n",
    "            return None\n",
    "\n",
    "        if self.is_last:\n",
    "            return self.input_vector\n",
    "\n",
    "        #the input is required to be in a row only\n",
    "        temp_input_matrix = np.reshape(self.input_vector, newshape=(1,self.num_nodes))\n",
    "\n",
    "        #input is consumed\n",
    "        self.input_vector = None\n",
    "\n",
    "        output_matrix = np.dot(temp_input_matrix,self.weight_matrix)\n",
    "\n",
    "        return output_matrix.flatten()\n",
    "\n",
    "\n",
    "    def desc(self):\n",
    "        print(\"Number of Nodes: {}\\n Number of Nodes in next layer: {}\\nWeights:\\n {}\\n\"\n",
    "               .format(self.num_nodes, self.next_num_nodes,self.weight_matrix))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class n_ff_network:\n",
    "\n",
    "    def __init__(self, ff_layers, initial_weights = None, has_bias = None,actn_fxn = relu):\n",
    "\n",
    "        self.ff_layers = list(np.array(ff_layers).flatten())\n",
    "        self.num_layers = len(self.ff_layers)\n",
    "\n",
    "        self.n_layers = []\n",
    "        \n",
    "        if has_bias is None:\n",
    "            has_bias = [False for i in range(self.num_layers-1)]\n",
    "        elif np.asarray(has_bias).flatten().shape != (self.num_layers-1,):\n",
    "            logging.warning('has_bias is not of compatible size. Using default value')\n",
    "            has_bias = [False for i in range(self.num_layers-1)]\n",
    "        \n",
    "        if isinstance(initial_weights,(type(None),int,float)):\n",
    "            self.n_layers = [ layer(num_nodes = self.ff_layers[i], next_num_nodes= self.ff_layers[i+1],\n",
    "                                    actn_fxn = actn_fxn, initial_weight= initial_weights, has_bias = has_bias[i])\n",
    "                               for i in range(self.num_layers - 1)]\n",
    "\n",
    "        elif initial_weights.shape[0] == self.num_layers - 1:\n",
    "            self.n_layers = [ layer(num_nodes = self.ff_layers[i], next_num_nodes= self.ff_layers[i+1],\n",
    "                                    actn_fxn = actn_fxn, initial_weight= initial_weights[i], has_bias = has_bias[i])\n",
    "                               for i in range(self.num_layers - 1)]\n",
    "\n",
    "        self.n_layers.append(layer(num_nodes = self.ff_layers[self.num_layers-1],initial_weight=1,\n",
    "                                       next_num_nodes=self.ff_layers[self.num_layers-1], actn_fxn = actn_fxn, is_last = True,\n",
    "                                  has_bias = False))\n",
    "\n",
    "        self.n_layers[0].actn_fxn = identity\n",
    "        self.input_vector = None\n",
    "        self.output_vector = None\n",
    "        \n",
    "\n",
    "    def send(self, input_vector):\n",
    "        \n",
    "        #checking if the input layer of neural network accepts the input_vector as a valid input vector\n",
    "        return_val = self.n_layers.send(input_vector)\n",
    "        \n",
    "        if return_val is None:\n",
    "            logging.critical('Input_vector incompatible. Cannot continue')\n",
    "            return None;\n",
    "        \n",
    "        #flattened to keep things simple\n",
    "        self.input_vector = np.array(input_vector).flatten()\n",
    "        \n",
    "        return self.input_vector\n",
    "    \n",
    "    def generate(self):\n",
    "\n",
    "        curr_input = self.input_vector\n",
    "\n",
    "        count = 0\n",
    "        for i in self.n_layers:\n",
    "            count +=1\n",
    "            i.send(curr_input)\n",
    "            curr_input = i.generate()\n",
    "\n",
    "        self.output_vector = curr_input\n",
    "\n",
    "        return self.output_vector\n",
    "\n",
    "    def desc(self):\n",
    "        for i in range(self.num_layers):\n",
    "            print(\"Layer {}\\n\".format(i+1))\n",
    "            self.n_layers[i].desc()\n",
    "\n",
    "    def show_network(self,comment=\"Neural Network\"):\n",
    "        dot = Digraph(comment)\n",
    "\n",
    "        \n",
    "        #legend\n",
    "        dot.attr(rankdir='LR',ranksep='4')\n",
    "        dot.node(name=\"input_layer\", rank = 'sink',xlabel=\"Input Layer\",label=\"\", fontsize = '12',style=\"filled\",color=\"green\",fixedsize = 'true', shape=\"square\",width=\"0.1\")\n",
    "        dot.node(name=\"output_layer\", xlabel=\"Output Layer\",label=\"\", fontsize = '12',style=\"filled\",color=\"red\",fixedsize = 'true', shape=\"square\",width=\"0.1\")\n",
    "        dot.node(name=\"hidden_layers\", xlabel=\"Hidden Layers\",label=\"\", fontsize = '12',style=\"filled\",color=\"grey\",fixedsize = 'true', shape=\"square\",width=\"0.1\")\n",
    "        \n",
    "        start = 0\n",
    "        for i in range(self.num_layers):\n",
    "            color = \"grey\"\n",
    "            if i == 0:\n",
    "                color = \"green\"\n",
    "            elif i == self.num_layers - 1:\n",
    "                color = \"red\"\n",
    "                \n",
    "            with dot.subgraph(name = 'cluster_'+str(i)) as subdot:\n",
    "                for j in range(self.n_layers[i].num_nodes):\n",
    "                    label_name = 'x' + str(i) + str(j)\n",
    "                    if self.n_layers[i].has_bias and j == 0:\n",
    "                        label_name = '1'\n",
    "                \n",
    "                    subdot.node(name = str(start),label = label_name,color=color, style=\"filled\",rankdir='TB')\n",
    "                    start +=1\n",
    "\n",
    "        start = 0\n",
    "        for i in range(self.num_layers-1):\n",
    "            curr_layer = self.n_layers[i]\n",
    "            next_start = start + curr_layer.num_nodes\n",
    "\n",
    "            tail = start\n",
    "            head = next_start\n",
    "            for j in range(curr_layer.num_nodes):\n",
    "                head = next_start\n",
    "                for k in range(curr_layer.next_num_nodes):\n",
    "                    dot.edge(tail_name = str(tail),head_name = str(head), label = str(curr_layer.weight_matrix[j][k]))\n",
    "                    head += 1\n",
    "                tail += 1\n",
    "\n",
    "            start = next_start\n",
    "\n",
    "        #dot.render('./'+comment+'.png', view = True)\n",
    "        dot.view()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = n_ff_network([9,4,3,1],has_bias=[False, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.show_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1+True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1+False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = Digraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot.node(\"kame\",label=\"hame\",fill=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:An output layer cannot have a bias. Cannot Continue\n",
      "\n",
      "CRITICAL:root:A layer cannot have only one node that is a bias\n",
      "\n",
      "CRITICAL:root:A layer cannot have only one node that is a bias\n",
      "\n",
      "CRITICAL:root:A layer cannot have only one node that is a bias\n",
      "\n",
      "CRITICAL:root:A layer cannot have only one node that is a bias\n",
      "\n",
      "CRITICAL:root:A layer cannot have only one node that is a bias\n",
      "\n",
      "CRITICAL:root:A layer cannot have only one node that is a bias\n",
      "\n",
      "CRITICAL:root:A layer cannot have only one node that is a bias\n",
      "\n",
      "CRITICAL:root:A layer cannot have only one node that is a bias\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = open('./log.txt')\n",
    "for i in file:\n",
    "    if \"CRITICAL\" in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dot.subgraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

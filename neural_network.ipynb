{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from graphviz import Digraph\n",
    "import math\n",
    "\n",
    "logging.basicConfig(filename='./log.txt',level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer:\n",
    "\n",
    "    def __init__(self, num_nodes, actn_fxn, next_num_nodes = 1, initial_weight = None, is_last = False, has_bias = False):\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.actn_fxn  = actn_fxn\n",
    "        self.next_num_nodes = next_num_nodes\n",
    "        self.is_last = is_last\n",
    "        self.weight_matrix = None\n",
    "        self.has_bias = has_bias\n",
    "\n",
    "        #cannot have a bias in output layer\n",
    "        if self.has_bias and self.is_last:\n",
    "            logging.critical(\"An output layer cannot have a bias. Cannot Continue\")\n",
    "            exit()\n",
    "        \n",
    "        #cannot have a bias only in a layer (it doesn't make sense)\n",
    "        if self.has_bias and self.num_nodes == 1:\n",
    "            logging.critical(\"A layer cannot have only one node that is a bias\")\n",
    "            exit()\n",
    "        \n",
    "        logging.debug(\"layer object initialized with \\n\\tNumber of nodes: {}\\n\\tActivation Function: {}\\n\\tNext Number of Nodes: {}\\n\\tIs Last: {}\\n\".format(self.num_nodes,self.actn_fxn,self.next_num_nodes,self.is_last))\n",
    "        \n",
    "        if initial_weight is None:\n",
    "            logging.debug('Using default values for the weight matrix')\n",
    "            self.weight_matrix = np.zeros([self.num_nodes, self.next_num_nodes],dtype=float)\n",
    "\n",
    "        elif isinstance(initial_weight, (int,float)):\n",
    "            logging.debug(\"Using the weight value for all weights in the layer\")\n",
    "            self.weight_matrix = (np.zeros(shape = (self.num_nodes, self.next_num_nodes),dtype=float))\n",
    "            self.weight_matrix.fill(float(initial_weight))\n",
    "\n",
    "        elif initial_weight.shape == (self.num_nodes, self.next_num_nodes):\n",
    "            logging.debug(\"Using user provided values\")\n",
    "            self.weight_matrix = np.array(initial_weight, dtype = float)\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            logging.warning('Weight matrix provided is of incompatible size. Using default values')\n",
    "            self.weight_matrix = np.zeros(shape = (self.num_nodes,self.next_num_nodes),dtype = float)\n",
    "\n",
    "        self.input_vector = None\n",
    "\n",
    "    def send(self, input_vector):\n",
    "\n",
    "        #flattened to keep things simple\n",
    "        input_vector = np.array(input_vector).flatten()\n",
    "\n",
    "        suppossed_input_vector_size = self.num_nodes\n",
    "        \n",
    "        if self.has_bias:\n",
    "            suppossed_input_vector_size -= 1\n",
    "            \n",
    "        if input_vector.shape != (suppossed_input_vector_size,):\n",
    "            logging.critical(\"Input vector not of desired size. Cannot continue!\")\n",
    "            return None\n",
    "\n",
    "        temp = []\n",
    "        if self.has_bias:\n",
    "            temp.append(1)\n",
    "        \n",
    "        for i in input_vector:\n",
    "            temp.append(self.actn_fxn(i))\n",
    "        self.input_vector = np.asarray([ self.actn_fxn(i) for i in input_vector ])\n",
    "\n",
    "        return self.input_vector\n",
    "\n",
    "    def generate(self):\n",
    "\n",
    "        if self.input_vector is None:\n",
    "            logging.critical(\"Input not yet provided. Cannot continue!\")\n",
    "            return None\n",
    "\n",
    "        if self.is_last:\n",
    "            return self.input_vector\n",
    "\n",
    "        #the input is required to be in a row only\n",
    "        temp_input_matrix = np.reshape(self.input_vector, newshape=(1,self.num_nodes))\n",
    "\n",
    "        #input is consumed\n",
    "        self.input_vector = None\n",
    "\n",
    "        output_matrix = np.dot(temp_input_matrix,self.weight_matrix)\n",
    "\n",
    "        return output_matrix.flatten()\n",
    "\n",
    "\n",
    "    def desc(self):\n",
    "        print(\"Number of Nodes: {}\\n Number of Nodes in next layer: {}\\nWeights:\\n {}\\n\"\n",
    "               .format(self.num_nodes, self.next_num_nodes,self.weight_matrix))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class n_ff_network:\n",
    "\n",
    "    def __init__(self, ff_layers, initial_weights = None, has_bias = None, actn_fxn : str = 'relu'):\n",
    "\n",
    "        self.ff_layers = list(np.array(ff_layers).flatten())\n",
    "        self.num_layers = len(self.ff_layers)\n",
    "\n",
    "        if self.num_layers < 2:\n",
    "            logging.critical('Neural Network cannot have less than two layers. Cannot Continue')\n",
    "            exit()\n",
    "        \n",
    "        self.n_layers = []\n",
    "        \n",
    "        self.actn_fxns = {}\n",
    "        \n",
    "        #default activation functions\n",
    "        self.actn_fxns['relu'] = self.relu\n",
    "        self.actn_fxns['identity'] = self.identity\n",
    "        self.actn_fxns['step'] = self.step\n",
    "        self.actn_fxns['sigmoid_bin'] = self.sigmoid_binary\n",
    "        self.actn_fxns['sigmoid_bipo'] = self.sigmoid_bipolar\n",
    "        \n",
    "        #default activation function parameter\n",
    "        self.theta = 0\n",
    "        \n",
    "        try:\n",
    "            actn_fxn = self.actn_fxns[actn_fxn]\n",
    "        except KeyError:\n",
    "            logging.warning('{} Activation function not found. Using ReLu instead'.format(actn_fxn))\n",
    "            actn_fxn = self.actn_fxns['relu']\n",
    "            \n",
    "        #for future purposes\n",
    "        self.actn_fxn = actn_fxn\n",
    "        \n",
    "        if has_bias is None:\n",
    "            has_bias = [False for i in range(self.num_layers-1)]\n",
    "        elif np.asarray(has_bias).flatten().shape != (self.num_layers-1,):\n",
    "            logging.warning('has_bias is not of compatible size. Using default value')\n",
    "            has_bias = [False for i in range(self.num_layers-1)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if isinstance(initial_weights,(type(None),int,float)):\n",
    "            self.n_layers = [ layer(num_nodes = self.ff_layers[i], next_num_nodes= self.ff_layers[i+1],\n",
    "                                    actn_fxn = actn_fxn, initial_weight= initial_weights, has_bias = has_bias[i])\n",
    "                               for i in range(self.num_layers - 1)]\n",
    "\n",
    "        elif initial_weights.shape[0] == self.num_layers - 1:\n",
    "            self.n_layers = [ layer(num_nodes = self.ff_layers[i], next_num_nodes= self.ff_layers[i+1],\n",
    "                                    actn_fxn = actn_fxn, initial_weight= initial_weights[i], has_bias = has_bias[i])\n",
    "                               for i in range(self.num_layers - 1)]\n",
    "\n",
    "        self.n_layers.append(layer(num_nodes = self.ff_layers[self.num_layers-1],initial_weight=1,\n",
    "                                       next_num_nodes=self.ff_layers[self.num_layers-1], actn_fxn = actn_fxn, is_last = True,\n",
    "                                  has_bias = False))\n",
    "\n",
    "        self.n_layers[0].actn_fxn = identity\n",
    "        self.input_vector = None\n",
    "        self.output_vector = None\n",
    "        \n",
    "        #default learning algos\n",
    "        self.learning_algos = {}\n",
    "        self.learning_algos['hebb'] = self.hebb_learn\n",
    "        self.learning_algos['perceptron'] = self.perceptron_learn\n",
    "        self.learning_algos['delta'] = self.delta_learn\n",
    "        #self.learning_algos['backprop'] = self.backprop\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def identity(self,x):\n",
    "        return x\n",
    "    \n",
    "    def relu(self,x):\n",
    "        return max(0,x)\n",
    "    \n",
    "    def step(self,x):\n",
    "        \n",
    "        if x > self.theta:\n",
    "            return 1\n",
    "        elif x < self.theta:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def sigmoid_bipolar(self,x):\n",
    "        return self.sigmoid_binary(x) - 0.5\n",
    "    \n",
    "    def sigmoid_binary(self,x):\n",
    "        return  1/(1 + math.e**(-x))\n",
    "    \n",
    "    def send(self, input_vector):\n",
    "        \n",
    "        #checking if the input layer of neural network accepts the input_vector as a valid input vector\n",
    "        return_val = self.n_layers[0].send(input_vector)\n",
    "        \n",
    "        if return_val is None:\n",
    "            logging.critical('Input_vector incompatible. Cannot continue')\n",
    "            return None;\n",
    "        \n",
    "        #flattened to keep things simple\n",
    "        self.input_vector = np.array(input_vector).flatten()\n",
    "        \n",
    "        return self.input_vector\n",
    "    \n",
    "    def generate(self):\n",
    "\n",
    "        curr_input = self.input_vector\n",
    "\n",
    "        count = 0\n",
    "        for i in self.n_layers:\n",
    "            count +=1\n",
    "            i.send(curr_input)\n",
    "            curr_input = i.generate()\n",
    "\n",
    "        self.output_vector = curr_input\n",
    "\n",
    "        return self.output_vector\n",
    "\n",
    "    def desc(self):\n",
    "        for i in range(self.num_layers):\n",
    "            print(\"Layer {}\\n\".format(i+1))\n",
    "            self.n_layers[i].desc()\n",
    "\n",
    "    def show_network(self,comment=\"Neural Network\"):\n",
    "        dot = Digraph(comment)\n",
    "\n",
    "        \n",
    "        #legend\n",
    "        dot.attr(rankdir='LR',ranksep='4')\n",
    "        dot.node(name=\"input_layer\", rank = 'sink',xlabel=\"Input Layer\",label=\"\", fontsize = '12',style=\"filled\",color=\"green\",fixedsize = 'true', shape=\"square\",width=\"0.1\")\n",
    "        dot.node(name=\"output_layer\", xlabel=\"Output Layer\",label=\"\", fontsize = '12',style=\"filled\",color=\"red\",fixedsize = 'true', shape=\"square\",width=\"0.1\")\n",
    "        if self.num_layers > 2:\n",
    "            dot.node(name=\"hidden_layers\", xlabel=\"Hidden Layers\",label=\"\", fontsize = '12',style=\"filled\",color=\"grey\",fixedsize = 'true', shape=\"square\",width=\"0.1\")\n",
    "        \n",
    "        start = 0\n",
    "        for i in range(self.num_layers):\n",
    "            color = \"grey\"\n",
    "            if i == 0:\n",
    "                color = \"green\"\n",
    "            elif i == self.num_layers - 1:\n",
    "                color = \"red\"\n",
    "                \n",
    "            with dot.subgraph(name = 'cluster_'+str(i)) as subdot:\n",
    "                for j in range(self.n_layers[i].num_nodes):\n",
    "                    label_name = 'x' + str(i) + str(j)\n",
    "                    if self.n_layers[i].has_bias and j == 0:\n",
    "                        label_name = '1'\n",
    "                \n",
    "                    subdot.node(name = str(start),label = label_name,color=color, style=\"filled\",rankdir='TB')\n",
    "                    start +=1\n",
    "\n",
    "        start = 0\n",
    "        for i in range(self.num_layers-1):\n",
    "            curr_layer = self.n_layers[i]\n",
    "            next_start = start + curr_layer.num_nodes\n",
    "\n",
    "            tail = start\n",
    "            head = next_start\n",
    "            for j in range(curr_layer.num_nodes):\n",
    "                head = next_start\n",
    "                for k in range(curr_layer.next_num_nodes):\n",
    "                    dot.edge(tail_name = str(tail),head_name = str(head), label = str(curr_layer.weight_matrix[j][k]))\n",
    "                    head += 1\n",
    "                tail += 1\n",
    "\n",
    "            start = next_start\n",
    "\n",
    "        #dot.render('./'+comment+'.png', view = True)\n",
    "        dot.view()\n",
    "        \n",
    "    def delta_learn(self, arg_dict):\n",
    "        \n",
    "        try:\n",
    "            output_vector = arg_dict['output_vector']\n",
    "        except KeyError:\n",
    "            logging.critical('output_vector argument not provided. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            learning_rate = arg_dict['learning_rate']\n",
    "        except KeyError:\n",
    "            logging.info('learning_rate argument not provided. Using default {} value'.format(1))\n",
    "            learning_rate = 1\n",
    "            \n",
    "        if self.num_layers > 2:\n",
    "            logging.critical('Delta learning rule currently doesn\\'t support more than two layers. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        input_layer = self.n_layers[0]\n",
    "        \n",
    "        for i in range(len(output_vector)):\n",
    "            t = output_vector[i]\n",
    "            for j in range(input_layer.num_nodes):\n",
    "                input_layer.weight_matrix[j][i] += learning_rate * (t - self.output_vector[i])*self.input_vector[j]\n",
    "        \n",
    "        \n",
    "    def perceptron_learn(self, arg_dict):\n",
    "        \n",
    "        try:\n",
    "            output_vector = arg_dict['output_vector']\n",
    "        except KeyError:\n",
    "            logging.critical('output_vector argument not provided. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            learning_rate = arg_dict['learning_rate']\n",
    "        except KeyError:\n",
    "            logging.info('learning_rate argument not provided. Using default {} value'.format(1))\n",
    "            learning_rate = 1\n",
    "            \n",
    "        try:\n",
    "            self.theta = arg_dict['theta']\n",
    "        except KeyError:\n",
    "            logging.info('theta argument not provided. Using default {} value'.format(0))\n",
    "        \n",
    "        if self.actn_fxn != self.step:\n",
    "            logging.warning('Perceptron normally uses Step activation function. Currently using {} instead'.format(self.actn_fxn))\n",
    "        \n",
    "        if self.num_layers > 2:\n",
    "            logging.critical('Perceptron rule currently doesn\\'t support more than two layers. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        if np.equal(output_vector, self.output_vector):\n",
    "            logging.debug('Actual and calcualted output equal. Skipping')\n",
    "            return True\n",
    "        \n",
    "        input_layer = self.n_layers[0]\n",
    "        for i in range(len(output_vector)):\n",
    "            output = output_vector[i]\n",
    "            for j in range(input_layer.num_nodes):\n",
    "                input_layer.weight_matrix[j][i] += learning_rate*output*self.input_vector[j]\n",
    "        \n",
    "        return True\n",
    "            \n",
    "    def hebb_learn(self,arg_dict):\n",
    "        \n",
    "        try:\n",
    "            output_vector = arg_dict['output_vector']\n",
    "        \n",
    "        except KeyError:\n",
    "            logging.critical('output_vector argument not provided. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        #hebb only works when there are no hidden layers\n",
    "        if self.num_layers > 2:\n",
    "            logging.critical('Hebb cannot work with any hidden layers. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        input_layer = self.n_layers[0]\n",
    "        for i in range(len(output_vector)):\n",
    "            output = output_vector[i]\n",
    "            for j in range(input_layer.num_nodes):\n",
    "                input_layer.weight_matrix[j][i] += output*self.input_vector[j]\n",
    "        \n",
    "        \n",
    "    def learn(self, learning_algo: str, input_matrix, output_matrix, epochs = 1, **xargs):\n",
    "        \n",
    "        if learning_algo not in self.learning_algos.keys():\n",
    "            logging.critical('{} learning algorithm is not recoginized. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        input_matrix = np.array(input_matrix)\n",
    "        output_matrix = np.array(output_matrix)\n",
    "        \n",
    "        if len(input_matrix.shape) != 2 or len(output_matrix.shape) != 2:\n",
    "            logging.critical('Input and Output matrices should have only two dimensions. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        if input_matrix.shape[0] != output_matrix.shape[0]:\n",
    "            logging.critical('Input and Output matrices have incompatible dimensions. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        input_layer = self.n_layers[0]\n",
    "        output_layer = self.n_layers[self.num_layers-1]\n",
    "        \n",
    "        num_pairs = input_matrix.shape[0]\n",
    "        \n",
    "        if num_pairs < 1:\n",
    "            logging.critical('No input provided. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        if input_layer.send(input_matrix[0]) is None:\n",
    "            logging.critical('Input Matrix dimensions` incompatible. Cannot Continue')\n",
    "            return None\n",
    "        \n",
    "        if output_layer.send(output_matrix[0]) is None:\n",
    "            logging.critical('Output Matrix dimensions` incompatible. Cannot Continue')\n",
    "            \n",
    "        \n",
    "        for i in range(epochs):\n",
    "            for j in range(num_pairs):\n",
    "                self.send(input_matrix[j].flatten())\n",
    "                self.generate()\n",
    "                xargs['output_vector'] = output_matrix[j].flatten()\n",
    "                self.learning_algos[learning_algo](xargs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = n_ff_network([3,1],actn_fxn='identity',initial_weights=0.2)\n",
    "def transform_bipolar(val : list) -> list:                                      \n",
    "    ans = []                                                                    \n",
    "    for i in val:                                                               \n",
    "        if i:                                                                   \n",
    "            ans.append(1)                                                       \n",
    "        else:                                                                   \n",
    "            ans.append(-1)                                                      \n",
    "    return ans                                                                  \n",
    "                                                                                \n",
    "                                                                                \n",
    "                                                                                \n",
    "def generate(func):                                                             \n",
    "                                                                                \n",
    "    input_v = []                                                                \n",
    "    output_v = []                                                               \n",
    "                                                                                \n",
    "    x = True                                                                    \n",
    "    y = True                                                                    \n",
    "                                                                                \n",
    "    func = func.lower()                                                         \n",
    "                                                                                \n",
    "    for i in range(4):                                                          \n",
    "                                                                                \n",
    "        if func == 'and':                                                       \n",
    "            t = x and y                                                         \n",
    "        elif func == 'nand':                                                    \n",
    "            t = not (x and y)                                                   \n",
    "        elif func == 'or':                                                      \n",
    "            t = x or y                                                          \n",
    "        elif func == 'nor':                                                     \n",
    "            t = not ( x or y )                                                  \n",
    "        \n",
    "        elif func == 'andnot':\n",
    "            t = x and (not y)\n",
    "        else:                                                                   \n",
    "            raise Exception('not defined')                                      \n",
    "                                                                                \n",
    "        input_v.append(transform_bipolar([x,y,True]))                           \n",
    "        output_v.append(transform_bipolar([t]))                                 \n",
    "                                                                                \n",
    "        if not y:                                                               \n",
    "            x = not x                                                           \n",
    "        y = not y                                                               \n",
    "                                                                                \n",
    "                                                                                \n",
    "    return input_v, output_v                                                    \n",
    "\n",
    "input_matrix, output_matrix = generate('andnot')\n",
    "network.learn('delta',input_matrix, output_matrix, epochs = 6, learning_rate = 0.2)\n",
    "network.show_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.049\n"
     ]
    }
   ],
   "source": [
    "print(\"1.04949494949\"[:-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
